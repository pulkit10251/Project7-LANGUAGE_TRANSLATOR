{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENCE TO SEQUENCE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING ALL NECCESSERRY LIBRARIES\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x,axis=1,keepdims=True))\n",
    "    s = K.sum(e,axis=1,keepdims=True)\n",
    "    return e/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=40\n",
    "LATENT_DIM=256\n",
    "NUM_OF_SAMPLES=10000\n",
    "MAX_NUM_WORDS=20000\n",
    "EMBEDDING_DIM=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE WE WILL STORE THE DATA\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "target_texts_inputs=[] # for force teaching in decoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 4638: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-56cc94f612a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# CAPTURING THE DATA only 10000 sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./spa-eng/spa.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mNUM_OF_SAMPLES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 4638: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# CAPTURING THE DATA only 10000 sentences\n",
    "t=0\n",
    "for line in open(\"./spa-eng/spa.txt\"):\n",
    "    t+=1\n",
    "    if t > NUM_OF_SAMPLES:\n",
    "        break\n",
    "    \n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    a=line\n",
    "    input_text,translation,_=line.rstrip().split(\"\\t\")\n",
    "    target_text= translation + '<eos>'\n",
    "    target_text_input = '<sos>' + translation\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples :  10000\n"
     ]
    }
   ],
   "source": [
    "# no of samples\n",
    "print(\"No. of samples : \",len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our input sentences\n",
    "tokenizer_input=Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_input.fit_on_texts(input_texts)\n",
    "input_sequences=tokenizer_input.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word to idx mapping for each word\n",
    "word2idx_inputs=tokenizer_input.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words :  2146\n"
     ]
    }
   ],
   "source": [
    "# unique input words\n",
    "print(\"Unique words : \",len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length :  5\n"
     ]
    }
   ],
   "source": [
    "# MAXIMUM LENGTH IN INPUT SENTENCES\n",
    "max_len_input=max(len(s) for s in input_sequences)\n",
    "print(\"Maximum length : \",max_len_input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE OUTPUT SENTENCES\n",
    "tokenizer_outputs=Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_outputs.fit_on_texts(target_texts+target_texts_inputs)\n",
    "\n",
    "target_sequences=tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_input=tokenizer_outputs.texts_to_sequences(target_texts_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD 2 IDX MAPPING FOR EACH WORD\n",
    "word2idx_output=tokenizer_outputs.word_index\n",
    "num_words_output = len(word2idx_output) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words :  4521\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words : \",len(word2idx_output) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# MAXIMUM LENGTH OF TARGET SENTENCE\n",
    "max_len_target=max(len(a) for a in target_sequences)\n",
    "print(max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (10000, 5)\n",
      "encoder_inputs[0]: [ 0  0  0  0 15]\n"
     ]
    }
   ],
   "source": [
    "# PADDING FOR ENCODER\n",
    "encoder_inputs = pad_sequences(input_sequences,maxlen=max_len_input)\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_inputs.shape: (10000, 11)\n",
      "decoder_inputs[0]: [ 2 51  0  0  0  0  0  0  0  0  0]\n",
      "decoder_targets.shape: (10000, 11)\n",
      "decoder_targets[0]: [51  1  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# PADDING FOR DECODER \n",
    "decoder_inputs= pad_sequences(target_sequences_input, maxlen = max_len_target, padding = 'post')\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
    "\n",
    "\n",
    "decoder_targets=pad_sequences(target_sequences,maxlen = max_len_target, padding = 'post')\n",
    "print(\"decoder_targets.shape:\", decoder_targets.shape)\n",
    "print(\"decoder_targets[0]:\", decoder_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORING ALL WORDS FROM GLOVE TO DICT\n",
    "word2vec = {}\n",
    "\n",
    "with open(\"./glove6b100dtxt/glove.6B.100d.txt\",encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vector=np.asarray((values[1:]),dtype='float32')\n",
    "        word2vec[word]=vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE EMBEDDING MATRIX\n",
    "num_words = min(MAX_NUM_WORDS,len(word2idx_inputs)+1)\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector=word2vec.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11, 4522)\n"
     ]
    }
   ],
   "source": [
    "# ONE HOT ENCODING\n",
    "decoder_targets_onehot=to_categorical(decoder_targets)\n",
    "print(decoder_targets_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PREPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE EMBEDDING LAYER\n",
    "embedding_layer=Embedding(num_words,EMBEDDING_DIM,weights=[embedding_matrix],input_length=max_len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 11, 4522)\n"
     ]
    }
   ],
   "source": [
    "# SETTING UP ENCODER\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_sequences=True,\n",
    "  dropout=0.5 # dropout not available on gpu\n",
    "))\n",
    "encoder_outputs = encoder(x)\n",
    "\n",
    "# Set up the decoder - not so simple\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "\n",
    "# this word embedding will not use pre-trained vectors\n",
    "# although you could\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### Attention #########\n",
    "# Attention layers need to be global because\n",
    "# they will be repeated Ty times at the decoder\n",
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10, activation='tanh')\n",
    "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n",
    "\n",
    "def one_step_attention(h, st_1):\n",
    "    # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
    "    # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
    "\n",
    "    # copy s(t-1) Tx times\n",
    "    # now shape = (Tx, LATENT_DIM_DECODER)\n",
    "    st_1 = attn_repeat_layer(st_1)\n",
    "\n",
    "    # Concatenate all h(t)'s with s(t-1)\n",
    "    # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
    "    x = attn_concat_layer([h, st_1])\n",
    "\n",
    "    # Neural net first layer\n",
    "    x = attn_dense1(x)\n",
    "    # Neural net second layer with special softmax over time\n",
    "    alphas = attn_dense2(x)\n",
    "\n",
    "    # \"Dot\" the alphas and the h's\n",
    "    # Remember a.dot(b) = sum over a[t] * b[t]\n",
    "    context = attn_dot([alphas, h])\n",
    "    \n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "# define the rest of the decoder (after attention)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "\n",
    "initial_s = Input(shape=(LATENT_DIM,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)\n",
    "\n",
    "\n",
    "# Unlike previous seq2seq, we cannot get the output\n",
    "# all in one step\n",
    "# Instead we need to do Ty steps\n",
    "# And in each of those steps, we need to consider\n",
    "# all Tx h's\n",
    "\n",
    "# s, c will be re-assigned in each iteration of the loop\n",
    "s = initial_s\n",
    "c = initial_c\n",
    "\n",
    "# collect outputs in a list at first\n",
    "outputs = []\n",
    "for t in range(max_len_target): # Ty times\n",
    "    # get the context using attention\n",
    "    context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "    # we need a different layer for each time step\n",
    "    selector = Lambda(lambda x: x[:, t:t+1]) \n",
    "    \n",
    "    xt = selector(decoder_inputs_x)\n",
    "    # combine \n",
    "    decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "    # pass the combined [context, last word] into the LSTM\n",
    "    # along with [s, c]\n",
    "    # get the new [s, c] and output\n",
    "    o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "    # final dense layer to get next word prediction\n",
    "    decoder_outputs = decoder_dense(o)\n",
    "\n",
    "    outputs.append(decoder_outputs)\n",
    "\n",
    "\n",
    "# 'outputs' is now a list of length Ty\n",
    "# each element is of shape (batch size, output vocab size)\n",
    "# therefore if we simply stack all the outputs into 1 tensor\n",
    "# it would be of shape T x N x D\n",
    "# we would like it to be of shape N x T x D\n",
    "\n",
    "def stack_and_transpose(x):\n",
    "    # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "    x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
    "    x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
    "    return x\n",
    "\n",
    "# make it a layer\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)\n",
    "print(outputs.shape)\n",
    "\n",
    "# create the model\n",
    "model = Model(\n",
    "  inputs=[\n",
    "    encoder_inputs_placeholder,\n",
    "    decoder_inputs_placeholder,\n",
    "    initial_s, \n",
    "    initial_c,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 5, 100)       214700      input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 5, 512)       731136      embedding[7][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 5, 256)       0           s0[0][0]                         \n",
      "                                                                 lstm_13[0][1]                    \n",
      "                                                                 lstm_13[1][1]                    \n",
      "                                                                 lstm_13[2][1]                    \n",
      "                                                                 lstm_13[3][1]                    \n",
      "                                                                 lstm_13[4][1]                    \n",
      "                                                                 lstm_13[5][1]                    \n",
      "                                                                 lstm_13[6][1]                    \n",
      "                                                                 lstm_13[7][1]                    \n",
      "                                                                 lstm_13[8][1]                    \n",
      "                                                                 lstm_13[9][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 5, 768)       0           bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[0][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[1][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[2][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[3][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[4][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[5][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[6][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[7][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[8][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[9][0]            \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 repeat_vector_7[10][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 5, 10)        7690        concatenate_10[0][0]             \n",
      "                                                                 concatenate_10[1][0]             \n",
      "                                                                 concatenate_10[2][0]             \n",
      "                                                                 concatenate_10[3][0]             \n",
      "                                                                 concatenate_10[4][0]             \n",
      "                                                                 concatenate_10[5][0]             \n",
      "                                                                 concatenate_10[6][0]             \n",
      "                                                                 concatenate_10[7][0]             \n",
      "                                                                 concatenate_10[8][0]             \n",
      "                                                                 concatenate_10[9][0]             \n",
      "                                                                 concatenate_10[10][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 11)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 5, 1)         11          dense_18[0][0]                   \n",
      "                                                                 dense_18[1][0]                   \n",
      "                                                                 dense_18[2][0]                   \n",
      "                                                                 dense_18[3][0]                   \n",
      "                                                                 dense_18[4][0]                   \n",
      "                                                                 dense_18[5][0]                   \n",
      "                                                                 dense_18[6][0]                   \n",
      "                                                                 dense_18[7][0]                   \n",
      "                                                                 dense_18[8][0]                   \n",
      "                                                                 dense_18[9][0]                   \n",
      "                                                                 dense_18[10][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 11, 100)      452200      input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 1, 512)       0           dense_19[0][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[1][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[2][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[3][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[4][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[5][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[6][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[7][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[8][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[9][0]                   \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "                                                                 dense_19[10][0]                  \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 1, 612)       0           dot_6[0][0]                      \n",
      "                                                                 lambda[0][0]                     \n",
      "                                                                 dot_6[1][0]                      \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 dot_6[2][0]                      \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 dot_6[3][0]                      \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 dot_6[4][0]                      \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 dot_6[5][0]                      \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 dot_6[6][0]                      \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 dot_6[7][0]                      \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 dot_6[8][0]                      \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 dot_6[9][0]                      \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 dot_6[10][0]                     \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, 256), (None, 889856      concatenate_11[0][0]             \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 concatenate_11[1][0]             \n",
      "                                                                 lstm_13[0][1]                    \n",
      "                                                                 lstm_13[0][2]                    \n",
      "                                                                 concatenate_11[2][0]             \n",
      "                                                                 lstm_13[1][1]                    \n",
      "                                                                 lstm_13[1][2]                    \n",
      "                                                                 concatenate_11[3][0]             \n",
      "                                                                 lstm_13[2][1]                    \n",
      "                                                                 lstm_13[2][2]                    \n",
      "                                                                 concatenate_11[4][0]             \n",
      "                                                                 lstm_13[3][1]                    \n",
      "                                                                 lstm_13[3][2]                    \n",
      "                                                                 concatenate_11[5][0]             \n",
      "                                                                 lstm_13[4][1]                    \n",
      "                                                                 lstm_13[4][2]                    \n",
      "                                                                 concatenate_11[6][0]             \n",
      "                                                                 lstm_13[5][1]                    \n",
      "                                                                 lstm_13[5][2]                    \n",
      "                                                                 concatenate_11[7][0]             \n",
      "                                                                 lstm_13[6][1]                    \n",
      "                                                                 lstm_13[6][2]                    \n",
      "                                                                 concatenate_11[8][0]             \n",
      "                                                                 lstm_13[7][1]                    \n",
      "                                                                 lstm_13[7][2]                    \n",
      "                                                                 concatenate_11[9][0]             \n",
      "                                                                 lstm_13[8][1]                    \n",
      "                                                                 lstm_13[8][2]                    \n",
      "                                                                 concatenate_11[10][0]            \n",
      "                                                                 lstm_13[9][1]                    \n",
      "                                                                 lstm_13[9][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 1, 100)       0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 4522)         1162154     lstm_13[0][0]                    \n",
      "                                                                 lstm_13[1][0]                    \n",
      "                                                                 lstm_13[2][0]                    \n",
      "                                                                 lstm_13[3][0]                    \n",
      "                                                                 lstm_13[4][0]                    \n",
      "                                                                 lstm_13[5][0]                    \n",
      "                                                                 lstm_13[6][0]                    \n",
      "                                                                 lstm_13[7][0]                    \n",
      "                                                                 lstm_13[8][0]                    \n",
      "                                                                 lstm_13[9][0]                    \n",
      "                                                                 lstm_13[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 11, 4522)     0           dense_20[0][0]                   \n",
      "                                                                 dense_20[1][0]                   \n",
      "                                                                 dense_20[2][0]                   \n",
      "                                                                 dense_20[3][0]                   \n",
      "                                                                 dense_20[4][0]                   \n",
      "                                                                 dense_20[5][0]                   \n",
      "                                                                 dense_20[6][0]                   \n",
      "                                                                 dense_20[7][0]                   \n",
      "                                                                 dense_20[8][0]                   \n",
      "                                                                 dense_20[9][0]                   \n",
      "                                                                 dense_20[10][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,457,747\n",
      "Trainable params: 3,457,747\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"seq2seqweights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Make predictions #####\n",
    "# As with the poetry example, we need to create another model\n",
    "# that can take in the RNN state and previous word as input\n",
    "# and accept a T=1 sequence.\n",
    "\n",
    "# The encoder will be stand-alone\n",
    "# From this we will get our initial decoder hidden state\n",
    "# i.e. h(1), ..., h(Tx)\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
    "\n",
    "# next we define a T=1 decoder model\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# no need to loop over attention steps this time because there is only one step\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
    "\n",
    "# combine context with last word\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lstm and final dense\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)\n",
    "\n",
    "\n",
    "# note: we don't really need the final stack and tranpose\n",
    "# because there's only 1 output\n",
    "# it is already of size N x D\n",
    "# no need to make it 1 x N x D --> N x 1 x D\n",
    "\n",
    "\n",
    "\n",
    "# create the model object\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    # NOTE: tokenizer lower-cases all words\n",
    "    target_seq[0, 0] = word2idx_output['sos']\n",
    "\n",
    "    # if we get this we break\n",
    "    eos = word2idx_output['eos']\n",
    "\n",
    "\n",
    "    # [s, c] will be updated in each loop iteration\n",
    "    s = np.zeros((1, LATENT_DIM))\n",
    "    c = np.zeros((1, LATENT_DIM))\n",
    "\n",
    "\n",
    "    # Create the translation\n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "\n",
    "\n",
    "        # Get next word\n",
    "        idx = np.argmax(o.flatten())\n",
    "\n",
    "        # End sentence of EOS\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        # Update the decoder input\n",
    "        # which is just the word just generated\n",
    "        target_seq[0, 0] = idx\n",
    "\n",
    "    return ' '.join(output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input: I'm so unlucky!\n",
      "Translation: vã©rifie vã©rifie soit arnaque arnaque arnaque arnaque arnaque arnaque arnaque arnaque\n",
      "Original: Quelle poisse j'aiâ€¯!<eos>\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Do some test translations\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    translation_a = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', input_texts[i])\n",
    "    print('Translation:', translation_a)\n",
    "    print('Original:',target_texts[i])\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
