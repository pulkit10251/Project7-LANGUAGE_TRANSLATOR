{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENCE TO SEQUENCE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTING ALL NECCESSERRY LIBRARIES\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x,axis=1,keepdims=True))\n",
    "    s = K.sum(e,axis=1,keepdims=True)\n",
    "    return e/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=40\n",
    "LATENT_DIM=256\n",
    "NUM_OF_SAMPLES=10000\n",
    "MAX_NUM_WORDS=20000\n",
    "EMBEDDING_DIM=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE WE WILL STORE THE DATA\n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "target_texts_inputs=[] # for force teaching in decoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAPTURING THE DATA only 10000 sentences\n",
    "t=0\n",
    "for line in open(\"./spa-eng/spa.txt\",encoding=\"utf8\"):\n",
    "    t+=1\n",
    "    if t > NUM_OF_SAMPLES:\n",
    "        break\n",
    "    \n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "    a=line\n",
    "    input_text,translation,_=line.rstrip().split(\"\\t\")\n",
    "    target_text= translation + '<eos>'\n",
    "    target_text_input = '<sos>' + translation\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples :  10000\n"
     ]
    }
   ],
   "source": [
    "# no of samples\n",
    "print(\"No. of samples : \",len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our input sentences\n",
    "tokenizer_input=Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_input.fit_on_texts(input_texts)\n",
    "input_sequences=tokenizer_input.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word to idx mapping for each word\n",
    "word2idx_inputs=tokenizer_input.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words :  2350\n"
     ]
    }
   ],
   "source": [
    "# unique input words\n",
    "print(\"Unique words : \",len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length :  5\n"
     ]
    }
   ],
   "source": [
    "# MAXIMUM LENGTH IN INPUT SENTENCES\n",
    "max_len_input=max(len(s) for s in input_sequences)\n",
    "print(\"Maximum length : \",max_len_input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE OUTPUT SENTENCES\n",
    "tokenizer_outputs=Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_outputs.fit_on_texts(target_texts+target_texts_inputs)\n",
    "\n",
    "target_sequences=tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_input=tokenizer_outputs.texts_to_sequences(target_texts_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD 2 IDX MAPPING FOR EACH WORD\n",
    "word2idx_output=tokenizer_outputs.word_index\n",
    "num_words_output = len(word2idx_output) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words :  4980\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words : \",len(word2idx_output) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# MAXIMUM LENGTH OF TARGET SENTENCE\n",
    "max_len_target=max(len(a) for a in target_sequences)\n",
    "print(max_len_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs.shape: (10000, 5)\n",
      "encoder_inputs[0]: [ 0  0  0  0 13]\n"
     ]
    }
   ],
   "source": [
    "# PADDING FOR ENCODER\n",
    "encoder_inputs = pad_sequences(input_sequences,maxlen=max_len_input)\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_inputs.shape: (10000, 9)\n",
      "decoder_inputs[0]: [ 1 67  0  0  0  0  0  0  0]\n",
      "decoder_targets.shape: (10000, 9)\n",
      "decoder_targets[0]: [67  2  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# PADDING FOR DECODER \n",
    "decoder_inputs= pad_sequences(target_sequences_input, maxlen = max_len_target, padding = 'post')\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
    "\n",
    "\n",
    "decoder_targets=pad_sequences(target_sequences,maxlen = max_len_target, padding = 'post')\n",
    "print(\"decoder_targets.shape:\", decoder_targets.shape)\n",
    "print(\"decoder_targets[0]:\", decoder_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORING ALL WORDS FROM GLOVE TO DICT\n",
    "word2vec = {}\n",
    "\n",
    "with open(\"./glove6b100dtxt/glove.6B.100d.txt\",encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vector=np.asarray((values[1:]),dtype='float32')\n",
    "        word2vec[word]=vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE EMBEDDING MATRIX\n",
    "num_words = min(MAX_NUM_WORDS,len(word2idx_inputs)+1)\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector=word2vec.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9, 4981)\n"
     ]
    }
   ],
   "source": [
    "# ONE HOT ENCODING\n",
    "decoder_targets_onehot=to_categorical(decoder_targets)\n",
    "print(decoder_targets_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL PREPERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE EMBEDDING LAYER\n",
    "embedding_layer=Embedding(num_words,EMBEDDING_DIM,weights=[embedding_matrix],input_length=max_len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 9, 4981)\n"
     ]
    }
   ],
   "source": [
    "# SETTING UP ENCODER\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_sequences=True,\n",
    "  dropout=0.5 # dropout not available on gpu\n",
    "))\n",
    "encoder_outputs = encoder(x)\n",
    "\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### Attention #########\n",
    "# Attention layers need to be global because\n",
    "# they will be repeated Ty times at the decoder\n",
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10, activation='tanh')\n",
    "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]\n",
    "\n",
    "def one_step_attention(h, st_1):\n",
    "    # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
    "    # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
    "\n",
    "    # copy s(t-1) Tx times\n",
    "    # now shape = (Tx, LATENT_DIM_DECODER)\n",
    "    st_1 = attn_repeat_layer(st_1)\n",
    "\n",
    "    # Concatenate all h(t)'s with s(t-1)\n",
    "    # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
    "    x = attn_concat_layer([h, st_1])\n",
    "\n",
    "    # Neural net first layer\n",
    "    x = attn_dense1(x)\n",
    "    # Neural net second layer with special softmax over time\n",
    "    alphas = attn_dense2(x)\n",
    "\n",
    "    # \"Dot\" the alphas and the h's\n",
    "    # Remember a.dot(b) = sum over a[t] * b[t]\n",
    "    context = attn_dot([alphas, h])\n",
    "    \n",
    "\n",
    "    return context\n",
    "\n",
    "\n",
    "# define the rest of the decoder (after attention)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "\n",
    "initial_s = Input(shape=(LATENT_DIM,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)\n",
    "\n",
    "\n",
    "\n",
    "s = initial_s\n",
    "c = initial_c\n",
    "\n",
    "# collect outputs in a list at first\n",
    "outputs = []\n",
    "for t in range(max_len_target): # Ty times\n",
    "    # get the context using attention\n",
    "    context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "    # we need a different layer for each time step\n",
    "    selector = Lambda(lambda x: x[:, t:t+1]) \n",
    "    \n",
    "    xt = selector(decoder_inputs_x)\n",
    "    # combine \n",
    "    decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "    # pass the combined [context, last word] into the LSTM\n",
    "    # along with [s, c]\n",
    "    # get the new [s, c] and output\n",
    "    o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "    # final dense layer to get next word prediction\n",
    "    decoder_outputs = decoder_dense(o)\n",
    "\n",
    "    outputs.append(decoder_outputs)\n",
    "\n",
    "\n",
    "# 'outputs' is now a list of length Ty\n",
    "# each element is of shape (batch size, output vocab size)\n",
    "# therefore if we simply stack all the outputs into 1 tensor\n",
    "# it would be of shape T x N x D\n",
    "# we would like it to be of shape N x T x D\n",
    "\n",
    "def stack_and_transpose(x):\n",
    "    # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "    x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
    "    x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
    "    return x\n",
    "\n",
    "# make it a layer\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)\n",
    "print(outputs.shape)\n",
    "\n",
    "# create the model\n",
    "model = Model(\n",
    "  inputs=[\n",
    "    encoder_inputs_placeholder,\n",
    "    decoder_inputs_placeholder,\n",
    "    initial_s, \n",
    "    initial_c,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 5, 100)       235100      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 5, 512)       731136      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 5, 256)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[1][1]                     \n",
      "                                                                 lstm_1[2][1]                     \n",
      "                                                                 lstm_1[3][1]                     \n",
      "                                                                 lstm_1[4][1]                     \n",
      "                                                                 lstm_1[5][1]                     \n",
      "                                                                 lstm_1[6][1]                     \n",
      "                                                                 lstm_1[7][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 768)       0           bidirectional[0][0]              \n",
      "                                                                 repeat_vector[0][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[1][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[2][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[3][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[4][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[5][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[6][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[7][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[8][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5, 10)        7690        concatenate[0][0]                \n",
      "                                                                 concatenate[1][0]                \n",
      "                                                                 concatenate[2][0]                \n",
      "                                                                 concatenate[3][0]                \n",
      "                                                                 concatenate[4][0]                \n",
      "                                                                 concatenate[5][0]                \n",
      "                                                                 concatenate[6][0]                \n",
      "                                                                 concatenate[7][0]                \n",
      "                                                                 concatenate[8][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 9)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5, 1)         11          dense[0][0]                      \n",
      "                                                                 dense[1][0]                      \n",
      "                                                                 dense[2][0]                      \n",
      "                                                                 dense[3][0]                      \n",
      "                                                                 dense[4][0]                      \n",
      "                                                                 dense[5][0]                      \n",
      "                                                                 dense[6][0]                      \n",
      "                                                                 dense[7][0]                      \n",
      "                                                                 dense[8][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 9, 100)       498100      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 512)       0           dense_1[0][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 612)       0           dot[0][0]                        \n",
      "                                                                 lambda[0][0]                     \n",
      "                                                                 dot[1][0]                        \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 dot[2][0]                        \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 dot[3][0]                        \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 dot[4][0]                        \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 dot[5][0]                        \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 dot[6][0]                        \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 dot[7][0]                        \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 dot[8][0]                        \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 889856      concatenate_1[0][0]              \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 lstm_1[1][1]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 lstm_1[2][1]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 lstm_1[3][1]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 lstm_1[4][1]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 lstm_1[5][1]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 lstm_1[6][1]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 lstm_1[7][1]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 1, 100)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4981)         1280117     lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 9, 4981)      0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,642,010\n",
      "Trainable params: 3,642,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z = np.zeros((len(encoder_inputs), LATENT_DIM)) # initial [s, c]\\nr = model.fit(\\n  [encoder_inputs, decoder_inputs, z, z], decoder_targets_onehot,\\n  batch_size=BATCH_SIZE,\\n  epochs=EPOCHS,\\n)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"z = np.zeros((len(encoder_inputs), LATENT_DIM)) # initial [s, c]\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_onehot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    ")\"\"\"\n",
    "# CODE ALREADY TRAINED ON DATA ON GOOGLE COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Make predictions #####\n",
    "# As with the poetry example, we need to create another model\n",
    "# that can take in the RNN state and previous word as input\n",
    "# and accept a T=1 sequence.\n",
    "\n",
    "# The encoder will be stand-alone\n",
    "# From this we will get our initial decoder hidden state\n",
    "# i.e. h(1), ..., h(Tx)\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
    "\n",
    "# next we define a T=1 decoder model\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# no need to loop over attention steps this time because there is only one step\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
    "\n",
    "# combine context with last word\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lstm and final dense\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)\n",
    "\n",
    "\n",
    "# note: we don't really need the final stack and tranpose\n",
    "# because there's only 1 output\n",
    "# it is already of size N x D\n",
    "# no need to make it 1 x N x D --> N x 1 x D\n",
    "\n",
    "\n",
    "\n",
    "# create the model object\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    # NOTE: tokenizer lower-cases all words\n",
    "    target_seq[0, 0] = word2idx_output['sos']\n",
    "\n",
    "    # if we get this we break\n",
    "    eos = word2idx_output['eos']\n",
    "\n",
    "\n",
    "    # [s, c] will be updated in each loop iteration\n",
    "    s = np.zeros((1, LATENT_DIM))\n",
    "    c = np.zeros((1, LATENT_DIM))\n",
    "\n",
    "\n",
    "    # Create the translation\n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "\n",
    "\n",
    "        # Get next word\n",
    "        idx = np.argmax(o.flatten())\n",
    "\n",
    "        # End sentence of EOS\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "        # Update the decoder input\n",
    "        # which is just the word just generated\n",
    "        target_seq[0, 0] = idx\n",
    "\n",
    "    return ' '.join(output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "-\n",
      "Input: I blacked out.\n",
      "Translation: quedé inconsciente\n",
      "Original: Quedé inconsciente.<eos>\n",
      "Continue? [Y/n]y\n",
      "(1, 5)\n",
      "-\n",
      "Input: They're Russian.\n",
      "Translation: ellos son rusos\n",
      "Original: Ellos son rusos.<eos>\n",
      "Continue? [Y/n]y\n",
      "(1, 5)\n",
      "-\n",
      "Input: I will sue you.\n",
      "Translation: te demandaré\n",
      "Original: Te demandaré.<eos>\n",
      "Continue? [Y/n]y\n",
      "(1, 5)\n",
      "-\n",
      "Input: Let me in.\n",
      "Translation: déjame entrar\n",
      "Original: Déjame entrar.<eos>\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Do some test translations\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i:i+1]\n",
    "    print(input_seq.shape)\n",
    "    translation_a = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input:', input_texts[i])\n",
    "    print('Translation:', translation_a)\n",
    "    print('Original:',target_texts[i])\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith('n'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(string):\n",
    "    string_seq=tokenizer_input.texts_to_sequences([string])\n",
    "    string_seq=pad_sequences(string_seq,maxlen=max_len_input)\n",
    "    translation=decode_sequence(string_seq)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'déjame entrar'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(\"let me in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
